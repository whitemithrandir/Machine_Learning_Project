{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dask.dataframe._pyarrow_compat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Albert import Einstein \n",
    "from Stephen import Hawking \n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = Hawking()\n",
    "_ , tokenizer =instance.huggingface_model_load()\n",
    "model_hawking = instance.load_model(os.getenv(\"hawking_path\"))\n",
    "\n",
    "def hawking_answer(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda(0)\n",
    "    model_outputs = model_hawking.generate(input_ids=input_ids, generation_config=GenerationConfig(min_new_tokens= 40, num_beams=1, do_sample=True, top_p=0.5, top_k=0, temperature=0.5,repetition_penalty=2.5))\n",
    "    model_text_output = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "    return model_text_output\n",
    "\n",
    "instance_ = Einstein()\n",
    "_ , tokenizer =instance_.huggingface_model_load()\n",
    "model_eintein = instance_.load_model(os.getenv(\"eintein_path\"))\n",
    "\n",
    "def einstein_answer(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda(0)\n",
    "    model_outputs = model_eintein.generate(input_ids=input_ids, generation_config=GenerationConfig(min_new_tokens= 80, max_new_tokens=200, num_beams=1, do_sample=True, top_p=0.9, top_k=0, temperature=0.5,repetition_penalty=2.5))\n",
    "\n",
    "    model_text_output = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "    return model_text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Ask question based on answer:\\n\\nAnswer\\n\\nA black hole is a region in space where the gravitational force is so strong that even light cannot escape. This region may form as a result of the collapse of a star. Square holes absorb the matter around them, sucking everything, including light, into space. They are called black because even light cannot escape. Because light cannot withstand the gravitational force and is pulled in. Black holes are mysterious and complex phenomena because they are difficult to observe..\\n\\nQuestion\"\n",
    "prompt\n",
    "\n",
    "prompt1 = \"\\n\\nAsk question based on answer:\\n\\nAnswer\\n\\n{x}\\n\\nQuestion\"\n",
    "prompt2 = \"\\n\\nAnswer the following question:\\n\\n{x}\\n\\nAnswer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbnkr\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stephen:  What does the black hole absorb the matter around them?\n",
      "\n",
      "albert:  The black hole absorbs the matter around them by absorbing it through their own chambers. It is an important part of this process, and one can use these materials to make up for its lack of water or heat energy. In this case, a large area in the vicinity of the Black Hole contains many molecules that are composed of particles that form bonds between the surface of the white dwarf galaxy. These structures have been found to be highly resistant to sunlight but have also become extremely difficult to detect because they cannot support the presence of gravity or other objects. This means that the mass of a small black hole has fallen apart, which causes the darkest areas to collapse in the darkness. However, there are several ways to conserve and preserve the material around them: 1. A dense layer of green algae, such as an alpha-like structure that surrounds the surrounding region, may contain some kind of flaky gas, such as sulfuric acid, carbon dioxide, and nitrogen. Some\n",
      "\n",
      "stephen:  What is the mass of a small black hole?\n",
      "\n",
      "albert:  Small black hole is a small black hole that forms the mass of a large, dense and narrow-eyed object. It has a density of about 67% and an average mass of approximately 33%. The size of a small black hole can be determined using this method: 1. Larger holes, such as tiny ones, are formed by moving objects around in smaller spaces like tiny one or larger structures. This means that they are either flattened with two to three feet (2 mm) or shallower than them. 2. High velocity areas: A small black hole has a low speed range of between 2â€“3 km/s. These masses have high speeds, but it cannot reach these distances and its magnitude is beyond 24 mph. 3. Square formations: There is a low-speed phase in which an earthquake occurs, and an unstable area may occur. In some cases, this happens due to gravity fluctuations. 4. Particle radiation\n",
      "\n",
      "stephen:  Larger holes, such as tiny ones, are formed by moving objects around in smaller spaces like\n",
      "\n",
      "albert:  Larger holes are formed by moving objects around in smaller spaces like the space between a hole and an object, such as a small one. The largest hole is called \"the last hole\" or \"small ones\", which means they can be found on large scales. They form in bigger places like a square (or rectangle) with larger stars. These shapes include: 1. A big single star or giant bird fly, known as a tiny one, may have been placed inside a huge rock-like structure that has been moved to other areas in less than 10 feet of surface. This phenomenon occurs when the particles move around in tiny holes. 2. An outer layer (such as a flatter pile) of dust (including some sort of debris) into smaller surfaces like Earth's crust or its surrounding environment. In this case, there is no inner layers, but the shape of the planet is much wider than it was previously known. 3. Space-sized structures: Small\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stephen = hawking_answer(prompt)\n",
    "print(\"stephen: \", stephen)\n",
    "print()\n",
    "\n",
    "albert = einstein_answer(prompt2.format(x=stephen))\n",
    "print(\"albert: \", albert)\n",
    "print()\n",
    "\n",
    "stephen = hawking_answer(prompt1.format(x=albert))\n",
    "print(\"stephen: \", stephen)\n",
    "print()\n",
    "\n",
    "albert = einstein_answer(prompt2.format(x=stephen))\n",
    "print(\"albert: \", albert)\n",
    "print()\n",
    "\n",
    "stephen = hawking_answer(prompt1.format(x=albert))\n",
    "print(\"stephen: \", stephen)\n",
    "print()\n",
    "\n",
    "albert = einstein_answer(prompt2.format(x=stephen))\n",
    "print(\"albert: \", albert)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# model_name =\"sabankara/albert_einstein\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomLanguageModel:\n",
    "#     def __init__(self, model, tokenizer):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def predict(self, input_text):\n",
    "#         input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda(0)\n",
    "#         model_outputs = self.model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             generation_config=GenerationConfig(\n",
    "#                 min_new_tokens=80,\n",
    "#                 max_new_tokens=200,\n",
    "#                 num_beams=1,\n",
    "#                 do_sample=True,\n",
    "#                 top_p=0.9,\n",
    "#                 top_k=0,\n",
    "#                 temperature=0.5,\n",
    "#                 repetition_penalty=2.5\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         model_text_output = self.tokenizer.decode(\n",
    "#             model_outputs[0], skip_special_tokens=True\n",
    "#         )\n",
    "#         return model_text_output\n",
    "\n",
    "\n",
    "# custom_model = CustomLanguageModel(model=model_eintein, tokenizer=tokenizer)\n",
    "\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# conversation = ConversationChain(\n",
    "#     llm=custom_model, \n",
    "#     memory = memory,\n",
    "#     verbose=False\n",
    "# )\n",
    "# #NOTE k=1 hafÄ±zayÄ± sÄ±nÄ±rlandÄ±rdÄ±\n",
    "# conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "# # conversation.predict(input=\"What is 1+1?\")\n",
    "# # conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
