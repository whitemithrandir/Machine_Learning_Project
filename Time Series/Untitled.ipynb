{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e0343f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d089081a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Preview the result\n",
    "for val in dataset:\n",
    "    print(val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a74b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data\n",
    "dataset = dataset.window(size=5, shift=1)\n",
    "\n",
    "# Print the result\n",
    "for window_dataset in dataset:\n",
    "    print(window_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6012cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "for window_dataset in dataset:\n",
    "    print([item.numpy() for item in window_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1778655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(size=5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Print the result\n",
    "for window_dataset in dataset:\n",
    "    print([item.numpy() for item in window_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf581a",
   "metadata": {},
   "source": [
    "### Modeli daha sonra eğitirken, pencereleri Veri Kümesi yapısı yerine tensörler olarak hazırlamak isteyeceksiniz. Bunu, flat_map() yöntemine bir eşleme işlevi besleyerek yapabilirsiniz. Bu işlev her pencereye uygulanacak ve sonuçlar tek bir veri kümesinde düzleştirilecektir. Göstermek için, aşağıdaki kod bir pencerenin tüm öğelerini tek bir kümeye yerleştirecek ve ardından sonucu düzleştirecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b694075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windows by putting its elements in a single batch\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# Print the results\n",
    "for window in dataset:\n",
    "    print(window.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f4c4e",
   "metadata": {},
   "source": [
    "### Ardından, her pencerede etiketleri işaretlemek isteyeceksiniz. Bu alıştırma için bunu, her pencerenin son öğesini ilk dörde bölerek yapacaksınız. Bu, pencere dilimlemeyi tanımlayan bir lambda işlevi içeren map() yöntemiyle yapılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e60961d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [0 1 2 3]\n",
      "y =  4\n",
      "\n",
      "x =  [1 2 3 4]\n",
      "y =  5\n",
      "\n",
      "x =  [2 3 4 5]\n",
      "y =  6\n",
      "\n",
      "x =  [3 4 5 6]\n",
      "y =  7\n",
      "\n",
      "x =  [4 5 6 7]\n",
      "y =  8\n",
      "\n",
      "x =  [5 6 7 8]\n",
      "y =  9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windows by putting its elements in a single batch\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# Create tuples with features (first four elements of the window) and labels (last element)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# Print the results\n",
    "for x,y in dataset:\n",
    "    print(\"x = \", x.numpy())\n",
    "    print(\"y = \", y.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e67ea",
   "metadata": {},
   "source": [
    "### Modelinizi eğitirken sıra bias azaltmak için veri kümenizi karıştırmak iyi bir uygulamadır. Bu, sinir ağının girdilerin sırasına overfitting anlamına gelir ve sonuç olarak, test sırasında belirli bir sırayı görmediğinde iyi performans göstermez. Eğitim girdileri dizisinin ağı bu şekilde etkilemesini istemezsiniz, bu yüzden onları karıştırmak iyidir.\n",
    "\n",
    "### Bunu yapmak için shuffle() yöntemini kullanabilirsiniz. Bunun için buffer_size parametresi gereklidir ve dokümanda belirtildiği gibi, daha iyi karıştırma için toplam öğe sayısına eşit veya daha büyük bir sayı koymalısınız. Veri setindeki toplam pencere sayısının 6 olduğunu önceki hücrelerden görebiliriz, bu sayıyı veya daha fazlasını seçebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f7946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [1 2 3 4]\n",
      "y =  5\n",
      "\n",
      "x =  [2 3 4 5]\n",
      "y =  6\n",
      "\n",
      "x =  [4 5 6 7]\n",
      "y =  8\n",
      "\n",
      "x =  [0 1 2 3]\n",
      "y =  4\n",
      "\n",
      "x =  [3 4 5 6]\n",
      "y =  7\n",
      "\n",
      "x =  [5 6 7 8]\n",
      "y =  9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windows by putting its elements in a single batch\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# Create tuples with features (first four elements of the window) and labels (last element)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# Shuffle the windows\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "\n",
    "# Print the results\n",
    "for x,y in dataset:\n",
    "    print(\"x = \", x.numpy())\n",
    "    print(\"y = \", y.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5e725",
   "metadata": {},
   "source": [
    "### Son olarak, pencerelerinizi gruplar halinde gruplandırmak isteyeceksiniz. Bunu aşağıda gösterildiği gibi batch() yöntemiyle yapabilirsiniz. Parti boyutunu belirtmeniz yeterlidir ve bu sayıda pencere ile toplu bir veri seti döndürür. Pratik bir kural olarak, bir prefetch() adımı belirtmek de iyidir. Bu, model zaten eğitilirken yürütme süresini optimize eder. Aşağıda gösterildiği gibi bir ön getirme arabellek_boyutu 1 belirtildiğinde, Tensorflow, mevcut toplu iş model tarafından tüketilirken bir sonraki toplu işi önceden hazırlayacaktır (yani bir arabelleğe koyarak). Bununla ilgili daha fazla bilgiyi [buradan](https://towardsdatascience.com/optimising-your-input-pipeline-performance-with-tf-data-part-1-32e52a30cac4#Prefetching). okuyabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff6d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[3 4 5 6]\n",
      " [4 5 6 7]]\n",
      "y =  [7 8]\n",
      "\n",
      "x =  [[1 2 3 4]\n",
      " [2 3 4 5]]\n",
      "y =  [5 6]\n",
      "\n",
      "x =  [[0 1 2 3]\n",
      " [5 6 7 8]]\n",
      "y =  [4 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windows by putting its elements in a single batch\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# Create tuples with features (first four elements of the window) and labels (last element)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# Shuffle the windows\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "\n",
    "# Create batches of windows\n",
    "dataset = dataset.batch(2).prefetch(1)\n",
    "\n",
    "# Print the results\n",
    "for x,y in dataset:\n",
    "    print(\"x = \", x.numpy())\n",
    "    print(\"y = \", y.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8854a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonts[mxnet,pro]\n",
      "  Downloading gluonts-0.12.8-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 7.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[mxnet,pro]) (0.12.0)\n",
      "Collecting pydantic~=1.7\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm~=4.23 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[mxnet,pro]) (4.64.1)\n",
      "Requirement already satisfied: pandas<3,>=1.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[mxnet,pro]) (1.5.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[mxnet,pro]) (4.4.0)\n",
      "Requirement already satisfied: numpy~=1.16 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[mxnet,pro]) (1.23.5)\n",
      "Collecting pyarrow~=8.0\n",
      "  Downloading pyarrow-8.0.0-cp310-cp310-win_amd64.whl (17.9 MB)\n",
      "     ---------------------------------------- 17.9/17.9 MB 5.6 MB/s eta 0:00:00\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.12-cp310-none-win_amd64.whl (195 kB)\n",
      "     ------------------------------------- 195.1/195.1 kB 12.3 MB/s eta 0:00:00\n",
      "Collecting mxnet~=1.7\n",
      "  Downloading mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "     --------------------------------------- 33.1/33.1 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting requests<2.19.0,>=2.18.4\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.7/88.7 kB ? eta 0:00:00\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting numpy~=1.16\n",
      "  Downloading numpy-1.16.6.zip (5.1 MB)\n",
      "     ---------------------------------------- 5.1/5.1 MB 10.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas<3,>=1.0->gluonts[mxnet,pro]) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas<3,>=1.0->gluonts[mxnet,pro]) (2.8.2)\n",
      "Collecting pandas<3,>=1.0\n",
      "  Using cached pandas-2.0.1-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "  Downloading pandas-2.0.0-cp310-cp310-win_amd64.whl (11.2 MB)\n",
      "     ---------------------------------------- 11.2/11.2 MB 8.3 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.5.2-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     ---------------------------------------- 10.4/10.4 MB 7.6 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.5.1-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     ---------------------------------------- 10.4/10.4 MB 9.1 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.5.0-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     --------------------------------------- 10.4/10.4 MB 10.9 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.4.4-cp310-cp310-win_amd64.whl (10.0 MB)\n",
      "     ---------------------------------------- 10.0/10.0 MB 9.5 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.4.3-cp310-cp310-win_amd64.whl (10.5 MB)\n",
      "     ---------------------------------------- 10.5/10.5 MB 4.4 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 3.9 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.4.1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 3.8 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.4.0-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 3.6 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.3.5-cp310-cp310-win_amd64.whl (10.2 MB)\n",
      "     --------------------------------------- 10.2/10.2 MB 11.2 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.3.4-cp310-cp310-win_amd64.whl (10.2 MB)\n",
      "     ---------------------------------------- 10.2/10.2 MB 4.5 MB/s eta 0:00:00\n",
      "  Downloading pandas-1.3.3.tar.gz (4.7 MB)\n",
      "     ---------------------------------------- 4.7/4.7 MB 11.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [241 lines of output]\n",
      "  Ignoring numpy: markers 'python_version == \"3.7\" and (platform_machine != \"arm64\" or platform_system != \"Darwin\") and platform_machine != \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and (platform_machine != \"arm64\" or platform_system != \"Darwin\") and platform_machine != \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.7\" and platform_machine == \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"aarch64\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\" and platform_machine == \"arm64\" and platform_system == \"Darwin\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.9\" and platform_machine == \"arm64\" and platform_system == \"Darwin\"' don't match your environment\n",
      "  Collecting setuptools>=51.0.0\n",
      "    Using cached setuptools-67.7.2-py3-none-any.whl (1.1 MB)\n",
      "  Collecting wheel\n",
      "    Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "  Collecting Cython<3,>=0.29.21\n",
      "    Downloading Cython-0.29.34-py2.py3-none-any.whl (988 kB)\n",
      "       -------------------------------------- 988.1/988.1 kB 6.3 MB/s eta 0:00:00\n",
      "  Collecting numpy==1.19.3\n",
      "    Downloading numpy-1.19.3.zip (7.3 MB)\n",
      "       ---------------------------------------- 7.3/7.3 MB 7.0 MB/s eta 0:00:00\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing metadata (pyproject.toml): started\n",
      "    Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    Preparing metadata (pyproject.toml) did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [202 lines of output]\n",
      "    setup.py:67: RuntimeWarning: NumPy 1.19.3 may not yet support Python 3.10.\n",
      "      warnings.warn(\n",
      "    Running from numpy source directory.\n",
      "    setup.py:480: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "      run_build = parse_setuppy_commands()\n",
      "    Processing numpy/random\\_bounded_integers.pxd.in\n",
      "    Processing numpy/random\\bit_generator.pyx\n",
      "    Processing numpy/random\\mtrand.pyx\n",
      "    Processing numpy/random\\_bounded_integers.pyx.in\n",
      "    Processing numpy/random\\_common.pyx\n",
      "    Processing numpy/random\\_generator.pyx\n",
      "    Processing numpy/random\\_mt19937.pyx\n",
      "    Processing numpy/random\\_pcg64.pyx\n",
      "    Processing numpy/random\\_philox.pyx\n",
      "    Processing numpy/random\\_sfc64.pyx\n",
      "    Cythonizing sources\n",
      "    blas_opt_info:\n",
      "    blas_mkl_info:\n",
      "    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "    customize MSVCCompiler\n",
      "      libraries mkl_rt not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    blis_info:\n",
      "      libraries blis not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_info:\n",
      "      libraries openblas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "    customize GnuFCompiler\n",
      "    Could not locate executable g77\n",
      "    Could not locate executable f77\n",
      "    customize IntelVisualFCompiler\n",
      "    Could not locate executable ifort\n",
      "    Could not locate executable ifl\n",
      "    customize AbsoftFCompiler\n",
      "    Could not locate executable f90\n",
      "    customize CompaqVisualFCompiler\n",
      "    Could not locate executable DF\n",
      "    customize IntelItaniumVisualFCompiler\n",
      "    Could not locate executable efl\n",
      "    customize Gnu95FCompiler\n",
      "    Could not locate executable gfortran\n",
      "    Could not locate executable f95\n",
      "    customize G95FCompiler\n",
      "    Could not locate executable g95\n",
      "    customize IntelEM64VisualFCompiler\n",
      "    customize IntelEM64TFCompiler\n",
      "    Could not locate executable efort\n",
      "    Could not locate executable efc\n",
      "    customize PGroupFlangCompiler\n",
      "    Could not locate executable flang\n",
      "    don't know how to compile Fortran code on platform 'nt'\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_blas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries tatlas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_blas_info:\n",
      "      libraries satlas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_blas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries ptf77blas,ptcblas,atlas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_blas_info:\n",
      "      libraries f77blas,cblas,atlas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    accelerate_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-install-my8tndli\\numpy_b410b3f40dcd46deb0de734e08c70db7\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Optimized (vendor) Blas libraries are not found.\n",
      "        Falls back to netlib Blas library which has worse performance.\n",
      "        A better performance should be easily gained by switching\n",
      "        Blas library.\n",
      "      if self._calc_info(blas):\n",
      "    blas_info:\n",
      "      libraries blas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-install-my8tndli\\numpy_b410b3f40dcd46deb0de734e08c70db7\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Blas (http://www.netlib.org/blas/) libraries not found.\n",
      "        Directories to search for the libraries can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [blas]) or by setting\n",
      "        the BLAS environment variable.\n",
      "      if self._calc_info(blas):\n",
      "    blas_src_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-install-my8tndli\\numpy_b410b3f40dcd46deb0de734e08c70db7\\numpy\\distutils\\system_info.py:1914: UserWarning:\n",
      "        Blas (http://www.netlib.org/blas/) sources not found.\n",
      "        Directories to search for the sources can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
      "        the BLAS_SRC environment variable.\n",
      "      if self._calc_info(blas):\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    non-existing path in 'numpy\\\\distutils': 'site.cfg'\n",
      "    lapack_opt_info:\n",
      "    lapack_mkl_info:\n",
      "      libraries mkl_rt not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_lapack_info:\n",
      "      libraries openblas not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    openblas_clapack_info:\n",
      "      libraries openblas,lapack not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    flame_info:\n",
      "      libraries flame not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries tatlas,tatlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries tatlas,tatlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "      libraries tatlas,tatlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_3_10_info:\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries satlas,satlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries satlas,satlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "      libraries satlas,satlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_threads_info:\n",
      "    Setting PTATLAS=ATLAS\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "      libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    atlas_info:\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries f77blas,cblas,atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\lib\n",
      "      libraries lapack_atlas not found in C:\\\n",
      "      libraries f77blas,cblas,atlas not found in C:\\\n",
      "      libraries lapack_atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "      libraries f77blas,cblas,atlas not found in C:\\Users\\saban.kara\\AppData\\Local\\anaconda3\\libs\n",
      "    <class 'numpy.distutils.system_info.atlas_info'>\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    lapack_info:\n",
      "      libraries lapack not found in ['C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\libs']\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-install-my8tndli\\numpy_b410b3f40dcd46deb0de734e08c70db7\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "        Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "        Directories to search for the libraries can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "        the LAPACK environment variable.\n",
      "      return getattr(self, '_calc_info_{}'.format(name))()\n",
      "    lapack_src_info:\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-install-my8tndli\\numpy_b410b3f40dcd46deb0de734e08c70db7\\numpy\\distutils\\system_info.py:1748: UserWarning:\n",
      "        Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "        Directories to search for the sources can be specified in the\n",
      "        numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "        the LAPACK_SRC environment variable.\n",
      "      return getattr(self, '_calc_info_{}'.format(name))()\n",
      "      NOT AVAILABLE\n",
      "  \n",
      "    numpy_linalg_lapack_lite:\n",
      "      FOUND:\n",
      "        language = c\n",
      "        define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]\n",
      "  \n",
      "    C:\\Users\\saban.kara\\AppData\\Local\\Temp\\pip-build-env-9x40fclp\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:275: UserWarning: Unknown distribution option: 'define_macros'\n",
      "      warnings.warn(msg)\n",
      "    running dist_info\n",
      "    running build_src\n",
      "    build_src\n",
      "    building py_modules sources\n",
      "    creating build\n",
      "    creating build\\src.win-amd64-3.10\n",
      "    creating build\\src.win-amd64-3.10\\numpy\n",
      "    creating build\\src.win-amd64-3.10\\numpy\\distutils\n",
      "    building library \"npymath\" sources\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: metadata-generation-failed\n",
      "  \n",
      "  Encountered error while generating package metadata.\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This is an issue with the package mentioned above, not pip.\n",
      "  hint: See above for details.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gluonts[mxnet,pro]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc2bed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonts[pro,torch]\n",
      "  Using cached gluonts-0.12.8-py3-none-any.whl (1.2 MB)\n",
      "Collecting pydantic~=1.7\n",
      "  Using cached pydantic-1.10.7-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: numpy~=1.16 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (1.23.5)\n",
      "Requirement already satisfied: pandas<3,>=1.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (1.5.3)\n",
      "Requirement already satisfied: tqdm~=4.23 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (4.4.0)\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (0.12.0)\n",
      "Collecting pytorch-lightning<3,>=1.5\n",
      "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
      "     -------------------------------------- 719.0/719.0 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting protobuf~=3.19.0\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.7/895.7 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy~=1.10 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (1.10.0)\n",
      "Requirement already satisfied: torch<3,>=1.9 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from gluonts[pro,torch]) (2.0.0)\n",
      "Collecting orjson\n",
      "  Using cached orjson-3.8.12-cp310-none-win_amd64.whl (195 kB)\n",
      "Collecting pyarrow~=8.0\n",
      "  Using cached pyarrow-8.0.0-cp310-cp310-win_amd64.whl (17.9 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas<3,>=1.0->gluonts[pro,torch]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas<3,>=1.0->gluonts[pro,torch]) (2022.7)\n",
      "Collecting lightning-utilities>=0.7.0\n",
      "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (22.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (6.0)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "     -------------------------------------- 519.2/519.2 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from torch<3,>=1.9->gluonts[pro,torch]) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from torch<3,>=1.9->gluonts[pro,torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from torch<3,>=1.9->gluonts[pro,torch]) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from torch<3,>=1.9->gluonts[pro,torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm~=4.23->gluonts[pro,torch]) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (2.28.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-win_amd64.whl (319 kB)\n",
      "     -------------------------------------- 319.8/319.8 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.0->gluonts[pro,torch]) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=1.9->gluonts[pro,torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch<3,>=1.9->gluonts[pro,torch]) (1.2.1)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (22.1.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.0/61.0 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saban.kara\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning<3,>=1.5->gluonts[pro,torch]) (1.26.14)\n",
      "Installing collected packages: pydantic, pyarrow, protobuf, orjson, multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, torchmetrics, gluonts, aiohttp, pytorch-lightning\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Eri\\u015fim engellendi: 'C:\\\\Users\\\\saban.kara\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\~yarrow\\\\arrow.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gluonts[torch,pro]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd2479b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gluonts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepAREstimator, Trainer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gluonts'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.mx import DeepAREstimator, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382255e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
