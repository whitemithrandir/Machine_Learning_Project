{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c17ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7251a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(m, seed=1, scale=0.7):\n",
    "    \"\"\" generate a data set based on a x^2 with added noise \"\"\"\n",
    "    c = 0\n",
    "    x_train = np.linspace(0,49,m)\n",
    "    np.random.seed(seed)\n",
    "    y_ideal = x_train**2 + c\n",
    "    y_train = y_ideal + scale * y_ideal*(np.random.sample((m,))-0.5)\n",
    "    x_ideal = x_train #for redraw when new data included in X\n",
    "    return x_train, y_train, x_ideal, y_ideal\n",
    "\n",
    "\n",
    "class lin_model:\n",
    "    def __init__(self, degree, regularization = False, lambda_=0):\n",
    "        if regularization:\n",
    "            self.linear_model = Ridge(alpha=lambda_)\n",
    "        else:\n",
    "            self.linear_model = LinearRegression()\n",
    "        self.poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X_train,y_train):\n",
    "        ''' just fits the data. mapping and scaling are not repeated '''\n",
    "        X_train_mapped = self.poly.fit_transform(X_train.reshape(-1,1))\n",
    "        X_train_mapped_scaled = self.scaler.fit_transform(X_train_mapped)\n",
    "        self.linear_model.fit(X_train_mapped_scaled, y_train )\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_mapped = self.poly.transform(X.reshape(-1,1))\n",
    "        X_mapped_scaled = self.scaler.transform(X_mapped)\n",
    "        yhat = self.linear_model.predict(X_mapped_scaled)\n",
    "        return(yhat)\n",
    "    \n",
    "    def mse(self, y, yhat):\n",
    "        err = mean_squared_error(y,yhat)/2   #sklean doesn't have div by 2\n",
    "        return (err)\n",
    "    \n",
    "    \n",
    "def plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "\n",
    "    ax.set_title(\"Poor Performance on Test Data\",fontsize = 12)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax.scatter(X_test, y_test,       color = \"blue\", label=\"test\")\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.plot(x, y_pred,  lw=0.5, label=f\"predicted, degree={degree}\")\n",
    "    ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plt_optimal_degree(X_train, y_train, X_cv, y_cv, x, y_pred, x_ideal, y_ideal,\n",
    "                       err_train,\n",
    "                       err_cv,\n",
    "                       optimal_degree, \n",
    "                       max_degree):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,1,figsize=(10,10))\n",
    "\n",
    "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[0].set_ylabel(\"y\")\n",
    "\n",
    "    ax[0].plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "    ax[0].scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax[0].scatter(X_cv, y_cv,       color = \"orange\", label=\"cv\")\n",
    "    ax[0].set_xlim(ax[0].get_xlim())\n",
    "    ax[0].set_ylim(ax[0].get_ylim())\n",
    "    for i in range(0,max_degree):\n",
    "        ax[0].plot(x, y_pred[:,i],  lw=0.5, label=f\"{i+1}\")\n",
    "    ax[0].legend(loc='upper left')\n",
    "\n",
    "    ax[1].set_title(\"error vs degree\",fontsize = 12)\n",
    "    cpts = list(range(1, max_degree+1))\n",
    "    ax[1].plot(cpts, err_train[0:], marker='o',label=\"train error\", lw=2,  color = \"blue\")\n",
    "    ax[1].plot(cpts, err_cv[0:],    marker='o',label=\"cv error\",  lw=2, color = \"orange\")\n",
    "    ax[1].set_ylim(*ax[1].get_ylim())\n",
    "    ax[1].axvline(optimal_degree, lw=1, color = \"magenta\")\n",
    "    ax[1].annotate(\"optimal degree\", xy=(optimal_degree,80000),xycoords='data',\n",
    "                xytext=(0.3, 0.8), textcoords='axes fraction', fontsize=10,\n",
    "                   arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\", \n",
    "                                   color=\"darkred\", lw=1))\n",
    "    ax[1].set_xlabel(\"degree\")\n",
    "    ax[1].set_ylabel(\"error\")\n",
    "    ax[1].legend()\n",
    "    fig.suptitle(\"Find Optimal Degree\",fontsize = 12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def plt_tune_regularization(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, optimal_reg_idx, lambda_range):\n",
    "    fig, ax = plt.subplots(2,1,figsize=(10,10))\n",
    "\n",
    "\n",
    "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[0].set_ylabel(\"y\")\n",
    "\n",
    "    ax[0].scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax[0].scatter(X_cv, y_cv,       color = \"orange\", label=\"cv\")\n",
    "    ax[0].set_xlim(ax[0].get_xlim())\n",
    "    ax[0].set_ylim(ax[0].get_ylim())\n",
    "#   ax[0].plot(x, y_pred[:,:],  lw=0.5, label=[f\"$\\lambda =${i}\" for i in lambda_range])\n",
    "    for i in (0,3,7,9):\n",
    "        ax[0].plot(x, y_pred[:,i],  lw=0.5, label=f\"$\\lambda =${lambda_range[i]}\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].set_title(\"error vs regularization\",fontsize = 12)\n",
    "    ax[1].plot(lambda_range, err_train[:], label=\"train error\", color = \"blue\")\n",
    "    ax[1].plot(lambda_range, err_cv[:],    label=\"cv error\",    color = \"orange\")\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_ylim(*ax[1].get_ylim())\n",
    "    opt_x = lambda_range[optimal_reg_idx]\n",
    "    ax[1].vlines(opt_x, *ax[1].get_ylim(), color = \"black\", lw=1)\n",
    "    ax[1].annotate(\"optimal lambda\", (opt_x,150000), xytext=(-80,10), textcoords=\"offset points\",\n",
    "                  arrowprops={'arrowstyle':'simple'})\n",
    "    ax[1].set_xlabel(\"regularization (lambda)\")\n",
    "    ax[1].set_ylabel(\"error\")\n",
    "    fig.suptitle(\"Tuning Regularization\",fontsize = 12)\n",
    "    ax[1].text(0.05,0.44,\"High\\nVariance\",fontsize=12, ha='left',transform=ax[1].transAxes,color = \"blue\")\n",
    "    ax[1].text(0.95,0.44,\"High\\nBias\",    fontsize=12, ha='right',transform=ax[1].transAxes,color = \"blue\")\n",
    "    ax[1].legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def tune_m():\n",
    "    \"\"\" tune the number of examples to reduce overfitting \"\"\"\n",
    "    m = 50\n",
    "    m_range = np.array(m*np.arange(1,16))\n",
    "    num_steps = m_range.shape[0]\n",
    "    degree = 16\n",
    "    err_train = np.zeros(num_steps)     \n",
    "    err_cv = np.zeros(num_steps)        \n",
    "    y_pred = np.zeros((100,num_steps))     \n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        X, y, y_ideal, x_ideal = gen_data(m_range[i],5,0.7)\n",
    "        x = np.linspace(0,int(X.max()),100)  \n",
    "        X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)\n",
    "\n",
    "        lmodel = lin_model(degree)  # no regularization\n",
    "        lmodel.fit(X_train, y_train)\n",
    "        yhat = lmodel.predict(X_train)\n",
    "        err_train[i] = lmodel.mse(y_train, yhat)\n",
    "        yhat = lmodel.predict(X_cv)\n",
    "        err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "        y_pred[:,i] = lmodel.predict(x)\n",
    "    return(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range,degree)\n",
    "\n",
    "\n",
    "def plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,1,figsize=(10,10))\n",
    "    \n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[0].set_ylabel(\"y\")\n",
    "\n",
    "    ax[0].scatter(X_train, y_train, color = \"red\",           s=3, label=\"train\", alpha=0.4)\n",
    "    ax[0].scatter(X_cv, y_cv,       color = \"orange\", s=3, label=\"cv\",    alpha=0.4)\n",
    "    ax[0].set_xlim(ax[0].get_xlim())\n",
    "    ax[0].set_ylim(ax[0].get_ylim())\n",
    "    for i in range(0,len(m_range),3):\n",
    "        ax[0].plot(x, y_pred[:,i],  lw=1, label=f\"$m =${m_range[i]}\")\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].text(0.05,0.5,f\"degree = {degree}\", fontsize=10, ha='left',transform=ax[0].transAxes,color = \"blue\")\n",
    "\n",
    "    ax[1].set_title(\"error vs number of examples\",fontsize = 12)\n",
    "    ax[1].plot(m_range, err_train[:], label=\"train error\", color = \"blue\")\n",
    "    ax[1].plot(m_range, err_cv[:],    label=\"cv error\",    color = \"orange\")\n",
    "    ax[1].set_xlabel(\"Number of Examples (m)\")\n",
    "    ax[1].set_ylabel(\"error\")\n",
    "    fig.suptitle(\"Tuning number of examples\",fontsize = 12)\n",
    "    ax[1].text(0.05,0.5,\"High\\nVariance\",        fontsize=12, ha='left',transform=ax[1].transAxes,color = \"blue\")\n",
    "    ax[1].text(0.95,0.5,\"Good \\nGeneralization\", fontsize=12, ha='right',transform=ax[1].transAxes,color = \"blue\")\n",
    "    ax[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    \n",
    "\n",
    "def plt_train_eq_dist(X_train,y_train,classes, X_cv,   y_cv, centers, std):\n",
    "    css = np.unique(y_train)\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "    plt_mc_data(ax[0], X_train,y_train,classes, map=dkcolors_map, legend=True, size=50)\n",
    "    plt_mc_data(ax[0], X_cv,   y_cv,   classes, map=ltcolors_map, legend=True, m=\"<\")\n",
    "    ax[0].set_title(\"Training, CV Data\")\n",
    "    for c in css:\n",
    "        circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n",
    "        ax[0].add_patch(circ)\n",
    "\n",
    "\n",
    "    #make a model for plotting routines to call\n",
    "    cat_predict = lambda pt: recat(pt.reshape(1,2), centers)\n",
    "    plot_cat_decision_boundary(ax[1], X_train, cat_predict,  vector=False, color = \"magenta\", lw=0.75)\n",
    "    ax[1].set_title(\"ideal performance\", fontsize=14)\n",
    "\n",
    "    #add the original data to the decison boundary\n",
    "    plt_mc_data(ax[1], X_train,y_train, classes, map=dkcolors_map, legend=True, size=50)\n",
    "    ax[1].set_xlabel('x0') ; ax[1].set_ylabel(\"x1\");\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#Plot a multi-class categorical decision boundary\n",
    "# This version handles a non-vector prediction (adds a for-loop over points)\n",
    "def plot_cat_decision_boundary(ax, X,predict , class_labels=None, legend=False, vector=True, color='g', lw = 1):\n",
    "\n",
    "    # create a mesh to points to plot\n",
    "    pad = 0.5\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "    h = max(x_max-x_min, y_max-y_min)/200\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    #print(\"points\", points.shape)\n",
    "    #make predictions for each point in mesh\n",
    "    if vector:\n",
    "        Z = predict(points)\n",
    "    else:\n",
    "        Z = np.zeros((len(points),))\n",
    "        for i in range(len(points)):\n",
    "            Z[i] = predict(points[i].reshape(1,2))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    #contour plot highlights boundaries between values - classes in this case\n",
    "    ax.contour(xx, yy, Z, colors=color, linewidths=lw) \n",
    "    ax.axis('tight')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def recat(pt, origins):\n",
    "    \"\"\" categorize a point based on distance from origin of clusters \"\"\"\n",
    "    nclusters = len(origins)\n",
    "    min_dist = 10000\n",
    "    y_new = None\n",
    "    for j in range(nclusters):\n",
    "        temp = origins[j] - pt.reshape(2,)\n",
    "        #print(temp.shape,origins[j].shape)\n",
    "        dist = np.sqrt(np.dot(temp.T, temp))\n",
    "        if dist < min_dist:\n",
    "            y_new = j\n",
    "            min_dist = dist\n",
    "    return(y_new)\n",
    "    \n",
    "    \n",
    "    \n",
    "def gen_blobs():\n",
    "    classes = 6\n",
    "    m = 800\n",
    "    std = 0.4\n",
    "    centers = np.array([[-1, 0], [1, 0], [0, 1], [0, -1],  [-2,1],[-2,-1]])\n",
    "    X, y = make_blobs(n_samples=m, centers=centers, cluster_std=std, random_state=2, n_features=2)\n",
    "    return (X, y, centers, classes, std)\n",
    "\n",
    "\n",
    "\n",
    "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired, legend=False,size=50, m='o'):\n",
    "    for i in range(classes):\n",
    "        idx = np.where(y == i)\n",
    "        col = len(idx[0])*[i]\n",
    "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
    "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
    "                    c=col, vmin=0, vmax=map.N, cmap=map,\n",
    "                    s=size, label=label)\n",
    "    if legend: ax.legend()\n",
    "    ax.axis('equal')\n",
    "\n",
    "    \n",
    "dkcolors = plt.cm.Paired((1,3,7,9,5,11))\n",
    "ltcolors = plt.cm.Paired((0,2,6,8,4,10))\n",
    "dkcolors_map = mpl.colors.ListedColormap(dkcolors)\n",
    "ltcolors_map = mpl.colors.ListedColormap(ltcolors)\n",
    "\n",
    "# GRADED CELL: eval_cat_err\n",
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      cerr: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "  \n",
    "        if(yhat[i]!=y[i]):\n",
    "            incorrect+=1\n",
    "            \n",
    "    cerr=incorrect/m\n",
    "    return(cerr)\n",
    "\n",
    "\n",
    "def plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"\"):\n",
    "    #plot the decison boundary.\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "    plot_cat_decision_boundary(ax[0], X_train, model_predict,  vector=True)\n",
    "    ax[0].set_title(\"training data\", fontsize=14)\n",
    "\n",
    "    #add the original data to the decison boundary\n",
    "    plt_mc_data(ax[0], X_train,y_train, classes, map=dkcolors_map, legend=True, size=75)\n",
    "    ax[0].set_xlabel('x0') ; ax[0].set_ylabel(\"x1\");\n",
    "\n",
    "    plot_cat_decision_boundary(ax[1], X_train, model_predict,  vector=True)\n",
    "    ax[1].set_title(\"cross-validation data\", fontsize=14)\n",
    "    plt_mc_data(ax[1], X_cv,y_cv, classes, \n",
    "                map=ltcolors_map, legend=True, size=100, m='<')\n",
    "    ax[1].set_xlabel('x0') ; ax[1].set_ylabel(\"x1\"); \n",
    "    fig.suptitle(suptitle,fontsize = 12)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def eval_cat_err(y, yhat):\n",
    "\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:\n",
    "            incorrect += 1\n",
    "    err = incorrect/m\n",
    "    return(err)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
