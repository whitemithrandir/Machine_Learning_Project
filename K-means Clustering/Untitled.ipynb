{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565f62b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saban.kara.d\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SABANK~1.D\\AppData\\Local\\Temp/ipykernel_18356/2884515495.py:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  choc_data['maker_location'] = choc_data['maker_location']\\\n",
      "C:\\Users\\SABANK~1.D\\AppData\\Local\\Temp/ipykernel_18356/2884515495.py:74: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  choc_data['specific_origin'] = choc_data['specific_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
      "C:\\Users\\SABANK~1.D\\AppData\\Local\\Temp/ipykernel_18356/2884515495.py:83: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  choc_data['broad_origin'] = choc_data['broad_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maker</th>\n",
       "      <th>specific_origin</th>\n",
       "      <th>reference_number</th>\n",
       "      <th>review_date</th>\n",
       "      <th>cocoa_percent</th>\n",
       "      <th>maker_location</th>\n",
       "      <th>rating</th>\n",
       "      <th>bean_type</th>\n",
       "      <th>broad_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>1876</td>\n",
       "      <td>2016</td>\n",
       "      <td>63.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Atsane</td>\n",
       "      <td>1676</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Akata</td>\n",
       "      <td>1680</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Quilla</td>\n",
       "      <td>1704</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      maker specific_origin  reference_number  review_date  cocoa_percent  \\\n",
       "0  A. Morin     Agua Grande              1876         2016          63.00   \n",
       "1  A. Morin           Kpime              1676         2015          70.00   \n",
       "2  A. Morin          Atsane              1676         2015          70.00   \n",
       "3  A. Morin           Akata              1680         2015          70.00   \n",
       "4  A. Morin          Quilla              1704         2015          70.00   \n",
       "\n",
       "  maker_location  rating bean_type broad_origin  \n",
       "0         France    3.75     Blend     Sao Tome  \n",
       "1         France    2.75     Blend         Togo  \n",
       "2         France    3.00     Blend         Togo  \n",
       "3         France    3.50     Blend         Togo  \n",
       "4         France    3.50     Blend         Peru  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run to load and clean the dataset\n",
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.linalg as nla\n",
    "import pandas as pd\n",
    "import re\n",
    "import six\n",
    "from os.path import join\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# Set the output display to have one digit for decimal places and limit it to\n",
    "# printing 15 rows.\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_rows = 15\n",
    "\n",
    "choc_data = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/flavors_of_cacao.csv\", sep=\",\", encoding='latin-1')\n",
    "\n",
    "# We can rename the columns.\n",
    "choc_data.columns = ['maker', 'specific_origin', 'reference_number', 'review_date', 'cocoa_percent', 'maker_location', 'rating', 'bean_type', 'broad_origin']\n",
    "\n",
    "# choc_data.dtypes\n",
    "\n",
    "# Replace empty/null values with \"Blend\"\n",
    "choc_data['bean_type'] = choc_data['bean_type'].fillna('Blend')\n",
    "\n",
    "#@title Cast bean_type to string to remove leading 'u'\n",
    "choc_data['bean_type'] = choc_data['bean_type'].astype(str)\n",
    "choc_data['cocoa_percent'] = choc_data['cocoa_percent'].str.strip('%')\n",
    "choc_data['cocoa_percent'] = pd.to_numeric(choc_data['cocoa_percent'])\n",
    "\n",
    "#@title Correct spelling mistakes, and replace city with country name\n",
    "choc_data['maker_location'] = choc_data['maker_location']\\\n",
    ".str.replace('Amsterdam', 'Holland')\\\n",
    ".str.replace('U.K.', 'England')\\\n",
    ".str.replace('Niacragua', 'Nicaragua')\\\n",
    ".str.replace('Domincan Republic', 'Dominican Republic')\n",
    "\n",
    "# Adding this so that Holland and Netherlands map to the same country.\n",
    "choc_data['maker_location'] = choc_data['maker_location']\\\n",
    ".str.replace('Holland', 'Netherlands')\n",
    "\n",
    "def cleanup_spelling_abbrev(text):\n",
    "    replacements = [\n",
    "        ['-', ', '], ['/ ', ', '], ['/', ', '], ['\\(', ', '], [' and', ', '], [' &', ', '], ['\\)', ''],\n",
    "        ['Dom Rep|DR|Domin Rep|Dominican Rep,|Domincan Republic', 'Dominican Republic'],\n",
    "        ['Mad,|Mad$', 'Madagascar, '],\n",
    "        ['PNG', 'Papua New Guinea, '],\n",
    "        ['Guat,|Guat$', 'Guatemala, '],\n",
    "        ['Ven,|Ven$|Venez,|Venez$', 'Venezuela, '],\n",
    "        ['Ecu,|Ecu$|Ecuad,|Ecuad$', 'Ecuador, '],\n",
    "        ['Nic,|Nic$', 'Nicaragua, '],\n",
    "        ['Cost Rica', 'Costa Rica'],\n",
    "        ['Mex,|Mex$', 'Mexico, '],\n",
    "        ['Jam,|Jam$', 'Jamaica, '],\n",
    "        ['Haw,|Haw$', 'Hawaii, '],\n",
    "        ['Gre,|Gre$', 'Grenada, '],\n",
    "        ['Tri,|Tri$', 'Trinidad, '],\n",
    "        ['C Am', 'Central America'],\n",
    "        ['S America', 'South America'],\n",
    "        [', $', ''], [',  ', ', '], [', ,', ', '], ['\\xa0', ' '],[',\\s+', ','],\n",
    "        [' Bali', ',Bali']\n",
    "    ]\n",
    "    for i, j in replacements:\n",
    "        text = re.sub(i, j, text)\n",
    "    return text\n",
    "\n",
    "choc_data['specific_origin'] = choc_data['specific_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "#@title Cast specific_origin to string\n",
    "choc_data['specific_origin'] = choc_data['specific_origin'].astype(str)\n",
    "\n",
    "#@title Replace null-valued fields with the same value as for specific_origin\n",
    "choc_data['broad_origin'] = choc_data['broad_origin'].fillna(choc_data['specific_origin'])\n",
    "\n",
    "#@title Clean up spelling mistakes and deal with abbreviations\n",
    "choc_data['broad_origin'] = choc_data['broad_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "# Change 'Trinitario, Criollo' to \"Criollo, Trinitario\"\n",
    "# Check with choc_data['bean_type'].unique()\n",
    "choc_data.loc[choc_data['bean_type'].isin(['Trinitario, Criollo']),'bean_type'] = \"Criollo, Trinitario\"\n",
    "# Confirm with choc_data[choc_data['bean_type'].isin(['Trinitario, Criollo'])]\n",
    "\n",
    "# Fix chocolate maker names\n",
    "choc_data.loc[choc_data['maker']=='Shattel','maker'] = 'Shattell'\n",
    "choc_data['maker'] = choc_data['maker'].str.replace(u'Na\\xef\\xbf\\xbdve','Naive')\n",
    "\n",
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd46dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maker</th>\n",
       "      <th>specific_origin</th>\n",
       "      <th>cocoa_percent</th>\n",
       "      <th>maker_location</th>\n",
       "      <th>rating</th>\n",
       "      <th>bean_type</th>\n",
       "      <th>broad_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Agua Grande</td>\n",
       "      <td>63.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Sao Tome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Kpime</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Atsane</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Akata</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Morin</td>\n",
       "      <td>Quilla</td>\n",
       "      <td>70.00</td>\n",
       "      <td>France</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Blend</td>\n",
       "      <td>Peru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      maker specific_origin  cocoa_percent maker_location  rating bean_type  \\\n",
       "0  A. Morin     Agua Grande          63.00         France    3.75     Blend   \n",
       "1  A. Morin           Kpime          70.00         France    2.75     Blend   \n",
       "2  A. Morin          Atsane          70.00         France    3.00     Blend   \n",
       "3  A. Morin           Akata          70.00         France    3.50     Blend   \n",
       "4  A. Morin          Quilla          70.00         France    3.50     Blend   \n",
       "\n",
       "  broad_origin  \n",
       "0     Sao Tome  \n",
       "1         Togo  \n",
       "2         Togo  \n",
       "3         Togo  \n",
       "4         Peru  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choc_data.drop(columns=['review_date','reference_number'],inplace=True)\n",
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ba7dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 373)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m373\u001b[0m\n\u001b[1;33m    ax.set_xlim([1, num_iterations])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#@title Functions to Build and Train a Similarity DNN Model\n",
    "\n",
    "class SimilarityModel(object):\n",
    "  \"\"\"Class to build, train, and inspect a Similarity Model.\n",
    "\n",
    "  This class builds a deep neural network that maps a dataset of entities\n",
    "  with heterogenous features to an embedding space.\n",
    "  Given a dataset as a pandas dataframe, determine the model by specifying\n",
    "  the set of features used as input and as labels to the DNN, and the\n",
    "  size of each hidden layer. The data is mapped to the embedding space\n",
    "  in the last hidden layer.\n",
    "  \n",
    "  To build an auto-encoder, make the set of output features identical to the set\n",
    "  of input features. Alternatively, build a predictor by using a single feature\n",
    "  as the label. When using a single feature as a label, ensure\n",
    "  this feature is removed from the input, or add at least\n",
    "  one hidden layer of a sufficiently low dimension such that the model cannot\n",
    "  trivially learn the label.\n",
    "  Caveat: The total loss being minimized is a simple sum of losses for each\n",
    "    output label (plus the regularization). If the output feature set combines\n",
    "    sparse and dense features, the total loss is a sum of cross-entropy soft-max\n",
    "    losses with root mean squared error losses, potentially in different scales,\n",
    "    which could emphasis some output labels more than others.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               dataframe,\n",
    "               input_feature_names,\n",
    "               output_feature_names,\n",
    "               dense_feature_names,\n",
    "               sparse_input_feature_embedding_dims,\n",
    "               hidden_dims=[32],\n",
    "               l2_regularization=0.0,\n",
    "               use_bias=True,\n",
    "               batch_size=100,\n",
    "               inspect=False):\n",
    "    \"\"\"Build a similarity model.\n",
    "\n",
    "    Args:\n",
    "      dataframe: the pandas dataframe used to train and validate the model.\n",
    "      input_feature_names: list of strings, names of input feature columns.\n",
    "      output_feature_names: list of strings, names of output feature columns.\n",
    "      dense_feature_names: list of strings, names of feature columns that are\n",
    "        treated as dense. All other feature columns are treated as sparse.\n",
    "      sparse_input_feature_embedding_dims: dictionary that maps feature names to\n",
    "        ints, expressing the embedding dimension of each input feature. Any\n",
    "        sparse feature in input_feature_names must be in this dictionary.\n",
    "      hidden_dims: list of ints, dimensions of each hidden layer. These hidden\n",
    "        layers are not counting the first layer which is a concatenation of the\n",
    "        input embeddings and the dense input features. Hence, this list can be\n",
    "        empty, in which case the outputs of the network are directly connected\n",
    "        to the input embeddings and/or dense inputs.\n",
    "      use_bias: bool, if true, add a bias term to each hidden layer.\n",
    "      batch_size: int, batch size.\n",
    "      inspect: bool, if true, add each tensor of the model to the list of\n",
    "        tensors that are inspected.\n",
    "    \"\"\"\n",
    "    used_feature_names = tuple(\n",
    "        set(input_feature_names).union(output_feature_names))\n",
    "    sparse_feature_names = tuple(\n",
    "        set(used_feature_names).difference(dense_feature_names))\n",
    "    # Dictionary mapping each sparse feature column to its vocabulary.\n",
    "    ### sparse_feature_vocabs = { 'maker': [u'A. Morin', u'AMMA', ...], ... }\n",
    "    sparse_feature_vocabs = {\n",
    "        sfn: sorted(list(set(choc_data[sfn].values)))\n",
    "        for sfn in sparse_feature_names\n",
    "    }\n",
    "\n",
    "    # Sparse output features are mapped to ids via tf.feature_to_id, hence\n",
    "    # we need key-id pairs for these vocabularies.\n",
    "    sparse_output_feature_names = (\n",
    "        tuple(set(sparse_feature_names).intersection(output_feature_names)))\n",
    "    keys_and_values = {}\n",
    "    for fn in sparse_output_feature_names:\n",
    "        keys = tf.constant(\n",
    "            sparse_feature_vocabs[fn],\n",
    "            dtype=tf.string,\n",
    "            name='{}_vocab_keys'.format(fn))\n",
    "        values = tf.range(\n",
    "            len(sparse_feature_vocabs[fn]),\n",
    "            dtype=tf.int64,\n",
    "            name='{}_vocab_values'.format(fn))\n",
    "        keys_and_values[fn] = (keys, values)\n",
    "\n",
    "    # Class instance data members.\n",
    "    self._session = None\n",
    "    self._loss = None\n",
    "    self._metrics = {}\n",
    "    self._embeddings = None\n",
    "    self._vars_to_inspect = {}\n",
    "\n",
    "    \n",
    "    def split_dataframe(df, holdout_fraction=0.1):\n",
    "      \"\"\"\n",
    "      Splits a pandas dataframe into training and test sets.\n",
    "\n",
    "      Args:\n",
    "        df: the source pandas dataframe.\n",
    "        holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "\n",
    "      Returns:\n",
    "        A pair of non-overlapping pandas dataframe for training and holdout.\n",
    "      \"\"\"\n",
    "        test = df.sample(frac=holdout_fraction, replace=False)\n",
    "        train = df[~df.index.isin(test.index)]\n",
    "        return train, test\n",
    "\n",
    "    train_dataframe, test_dataframe = split_dataframe(dataframe)\n",
    "\n",
    "    def make_batch(dataframe, batch_size):\n",
    "      \"\"\"Creates a batch of examples.\n",
    "\n",
    "      Args:\n",
    "        dataframe: a panda dataframe with rows being examples and with\n",
    "          columns being feature columns.\n",
    "        batch_size: the batch size.\n",
    "\n",
    "      Returns:\n",
    "        A dictionary of tensors, keyed by their feature names.\n",
    "        Each tensor is of shape [batch_size]. Tensors for sparse features are of\n",
    "        strings, while tensors for dense features are of floats.\n",
    "      \"\"\"\n",
    "      used_features = {ufn: dataframe[ufn] for ufn in used_feature_names}\n",
    "      batch = (\n",
    "          tf.data.Dataset.from_tensor_slices(used_features).shuffle(1000)\n",
    "          .repeat().batch(batch_size).make_one_shot_iterator().get_next())\n",
    "      if inspect:\n",
    "        for k, v in six.iteritems(batch):\n",
    "          self._vars_to_inspect['input_%s' % k] = v\n",
    "      return batch\n",
    "\n",
    "    def generate_feature_columns(feature_names):\n",
    "      \"\"\"Creates the list of used feature columns.\n",
    "\n",
    "      Args:\n",
    "        feature_names: an iterable of strings with the names of the features for\n",
    "          which feature columns are generated.\n",
    "\n",
    "      Returns:\n",
    "        A dictionary, keyed by feature names, of _DenseColumn and\n",
    "        _NumericColumn.\n",
    "      \"\"\"\n",
    "      used_sparse_feature_names = (\n",
    "          tuple(set(sparse_feature_names).intersection(feature_names)))\n",
    "      used_dense_feature_names = (\n",
    "          tuple(set(dense_feature_names).intersection(feature_names)))\n",
    "      f_columns = {}\n",
    "      for sfn in used_sparse_feature_names:\n",
    "        sf_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key=sfn,\n",
    "            vocabulary_list=sparse_feature_vocabs[sfn],\n",
    "            num_oov_buckets=0)\n",
    "        f_columns[sfn] = tf.feature_column.embedding_column(\n",
    "            categorical_column=sf_column,\n",
    "            dimension=sparse_input_feature_embedding_dims[sfn],\n",
    "            combiner='mean',\n",
    "            initializer=tf.truncated_normal_initializer(stddev=.1))\n",
    "      for dfn in used_dense_feature_names:\n",
    "        f_columns[dfn] = tf.feature_column.numeric_column(dfn)\n",
    "      return f_columns\n",
    "\n",
    "    def create_tower(features, columns):\n",
    "      \"\"\"Creates the tower mapping features to embeddings.\n",
    "\n",
    "      Args:\n",
    "        features: a dictionary of tensors of shape [batch_size], keyed by\n",
    "          feature name. Sparse features are associated to tensors of strings,\n",
    "          while dense features are associated to tensors of floats.\n",
    "        columns: a dictionary, keyed by feature names, of _DenseColumn and\n",
    "          _NumericColumn.\n",
    "\n",
    "      Returns:\n",
    "        A pair of elements: hidden_layer and output_layer.\n",
    "          hidden_layer is a tensor of shape [batch_size, hidden_dims[-1]].\n",
    "          output_layer is a dictionary keyed by the output feature names, of\n",
    "            dictionaries {'labels': labels, 'logits': logits}.\n",
    "            Dense output features have both labels and logits as float tensors \n",
    "            of shape [batch_size, 1]. Sparse output features have labels as\n",
    "            string tensors of shape [batch_size, 1] and logits as float tensors\n",
    "            of shape [batch_size, len(sparse_feature_vocab)].\n",
    "      \"\"\"\n",
    "      # TODO: sanity check the arguments.\n",
    "      # Input features.\n",
    "      input_columns = [columns[fn] for fn in input_feature_names]\n",
    "      hidden_layer = tf.feature_column.input_layer(features, input_columns)\n",
    "      dense_input_feature_names = (\n",
    "          tuple(set(dense_feature_names).intersection(input_feature_names)))\n",
    "      input_dim = (\n",
    "          sum(sparse_input_feature_embedding_dims.values()) +\n",
    "          len(dense_input_feature_names))\n",
    "      for layer_idx, layer_output_dim in enumerate(hidden_dims):\n",
    "        w = tf.get_variable(\n",
    "            'hidden%d_w_' % layer_idx,\n",
    "            shape=[input_dim, layer_output_dim],\n",
    "            initializer=tf.truncated_normal_initializer(\n",
    "                stddev=1.0 / np.sqrt(layer_output_dim)))\n",
    "        if inspect:\n",
    "          self._vars_to_inspect['hidden%d_w_' % layer_idx] = w\n",
    "        hidden_layer = tf.matmul(hidden_layer, w)  # / 10.)\n",
    "        if inspect:\n",
    "          self._vars_to_inspect['hidden_layer_%d' % layer_idx] = hidden_layer\n",
    "        input_dim = layer_output_dim\n",
    "      # Output features.\n",
    "      output_layer = {}\n",
    "      for ofn in output_feature_names:\n",
    "        if ofn in sparse_feature_names:\n",
    "          feature_dim = len(sparse_feature_vocabs[ofn])\n",
    "        else:\n",
    "          feature_dim = 1\n",
    "        w = tf.get_variable(\n",
    "            'output_w_%s' % ofn,\n",
    "            shape=[input_dim, feature_dim],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=1.0 /\n",
    "                                                        np.sqrt(feature_dim)))\n",
    "        if inspect:\n",
    "          self._vars_to_inspect['output_w_%s' % ofn] = w\n",
    "        if use_bias:\n",
    "          bias = tf.get_variable(\n",
    "              'output_bias_%s' % ofn,\n",
    "              shape=[1, feature_dim],\n",
    "              initializer=tf.truncated_normal_initializer(stddev=1.0 /\n",
    "                                                          np.sqrt(feature_dim)))\n",
    "          if inspect:\n",
    "            self._vars_to_inspect['output_bias_%s' % ofn] = bias\n",
    "        else:\n",
    "          bias = tf.constant(0.0, shape=[1, feature_dim])\n",
    "        output_layer[ofn] = {\n",
    "            'labels':\n",
    "                features[ofn],\n",
    "            'logits':\n",
    "                tf.add(tf.matmul(hidden_layer, w), bias)  # w / 10.), bias)\n",
    "        }\n",
    "        if inspect:\n",
    "          self._vars_to_inspect['output_labels_%s' %\n",
    "                                ofn] = output_layer[ofn]['labels']\n",
    "          self._vars_to_inspect['output_logits_%s' %\n",
    "                                ofn] = output_layer[ofn]['logits']\n",
    "      return hidden_layer, output_layer\n",
    "\n",
    "    def similarity_loss(top_embeddings, output_layer):\n",
    "      \"\"\"Build the loss to be optimized.\n",
    "\n",
    "      Args:\n",
    "        top_embeddings: First element returned by create_tower.\n",
    "        output_layer: Second element returned by create_tower.\n",
    "\n",
    "      Returns:\n",
    "        total_loss: A tensor of shape [1] with the total loss to be optimized.\n",
    "        losses: A dictionary keyed by output feature names, of tensors of shape\n",
    "          [1] with the contribution to the loss of each output feature.\n",
    "      \"\"\"\n",
    "      losses = {}\n",
    "      total_loss = tf.scalar_mul(l2_regularization,\n",
    "                                 tf.nn.l2_loss(top_embeddings))\n",
    "      for fn, output in six.iteritems(output_layer):\n",
    "        if fn in sparse_feature_names:\n",
    "          losses[fn] = tf.reduce_mean(\n",
    "              tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                  logits=output['logits'],\n",
    "                  labels=tf.feature_to_id(\n",
    "                      output['labels'], keys_and_values=keys_and_values[fn])))\n",
    "        else:\n",
    "          losses[fn] = tf.sqrt(\n",
    "              tf.reduce_mean(\n",
    "                  tf.square(output['logits'] -\n",
    "                            tf.cast(output['labels'], tf.float32))))\n",
    "        total_loss += losses[fn]\n",
    "      return total_loss, losses\n",
    "\n",
    "    # Body of the constructor.\n",
    "    input_feature_columns = generate_feature_columns(input_feature_names)\n",
    "    # Train\n",
    "    with tf.variable_scope('model', reuse=False):\n",
    "      train_hidden_layer, train_output_layer = create_tower(\n",
    "          make_batch(train_dataframe, batch_size), input_feature_columns)\n",
    "      self._train_loss, train_losses = similarity_loss(train_hidden_layer,\n",
    "                                                       train_output_layer)\n",
    "    # Test\n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "      test_hidden_layer, test_output_layer = create_tower(\n",
    "          make_batch(test_dataframe, batch_size), input_feature_columns)\n",
    "      test_loss, test_losses = similarity_loss(test_hidden_layer,\n",
    "                                               test_output_layer)\n",
    "    # Whole dataframe to get final embeddings\n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "      self._hidden_layer, _ = create_tower(\n",
    "          make_batch(dataframe, dataframe.shape[0]), input_feature_columns)\n",
    "    # Metrics is a dictionary of dictionaries of dictionaries.\n",
    "    # The 3 levels are used as plots, line colors, and line styles respectively.\n",
    "    self._metrics = {\n",
    "        'total': {\n",
    "            'train': {'loss': self._train_loss},\n",
    "            'test': {'loss': test_loss}\n",
    "        },\n",
    "        'feature': {\n",
    "            'train': {'%s loss' % k: v for k, v in six.iteritems(train_losses)},\n",
    "            'test': {'%s loss' % k: v for k, v in six.iteritems(test_losses)}\n",
    "        }\n",
    "    }\n",
    "\n",
    "  def train(self,\n",
    "            num_iterations=30,\n",
    "            learning_rate=1.0,\n",
    "            plot_results=True,\n",
    "            optimizer=tf.train.GradientDescentOptimizer):\n",
    "    \"\"\"Trains the model.\n",
    "\n",
    "    Args:\n",
    "      num_iterations: int, the number of iterations to run.\n",
    "      learning_rate: float, the optimizer learning rate.\n",
    "      plot_results: bool, whether to plot the results at the end of training.\n",
    "      optimizer: tf.train.Optimizer, the optimizer to be used for training.\n",
    "    \"\"\"\n",
    "    with self._train_loss.graph.as_default():\n",
    "      opt = optimizer(learning_rate)\n",
    "      train_op = opt.minimize(self._train_loss)\n",
    "      opt_init_op = tf.variables_initializer(opt.variables())\n",
    "      if self._session is None:\n",
    "        self._session = tf.Session()\n",
    "        with self._session.as_default():\n",
    "          self._session.run(tf.global_variables_initializer())\n",
    "          self._session.run(tf.local_variables_initializer())\n",
    "          self._session.run(tf.tables_initializer())\n",
    "          tf.train.start_queue_runners()\n",
    "\n",
    "    with self._session.as_default():\n",
    "      self._session.run(opt_init_op)\n",
    "      if plot_results:  \n",
    "        iterations = []\n",
    "        metrics_vals = {k0: {k1: {k2: []\n",
    "                                  for k2 in v1}\n",
    "                             for k1, v1 in six.iteritems(v0)}\n",
    "                        for k0, v0 in six.iteritems(self._metrics)}\n",
    "\n",
    "      # Train and append results.\n",
    "      for i in range(num_iterations + 1):\n",
    "        _, results = self._session.run((train_op, self._metrics))\n",
    "\n",
    "        # Printing the 1 liner with losses.\n",
    "        if (i % 10 == 0) or i == num_iterations:\n",
    "          print('\\riteration%6d,   ' % i + ',   '.join(\n",
    "              ['%s %s %s: %7.3f' % (k0, k1, k2, v2)\n",
    "               for k0, v0 in six.iteritems(results)\n",
    "               for k1, v1 in six.iteritems(v0)\n",
    "               for k2, v2 in six.iteritems(v1)])\n",
    "                , end=\" \"\n",
    "               )\n",
    "          if plot_results:\n",
    "            iterations.append(i)\n",
    "            for k0, v0 in six.iteritems(results):\n",
    "              for k1, v1 in six.iteritems(v0):\n",
    "                for k2, v2 in six.iteritems(v1):\n",
    "                  metrics_vals[k0][k1][k2].append(results[k0][k1][k2])\n",
    "\n",
    "      # Feedforward the entire dataframe to get all the embeddings.\n",
    "      self._embeddings = self._session.run(self._hidden_layer)\n",
    "\n",
    "      # Plot the losses and embeddings.\n",
    "      if plot_results:\n",
    "        num_subplots = len(metrics_vals) + 1\n",
    "        colors = 10 * ('red', 'blue', 'black', 'green')\n",
    "        styles = 10 * ('-', '--', '-.', ':')\n",
    "        # Plot the metrics.\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(num_subplots*10, 8)\n",
    "        for i0, (k0, v0) in enumerate(six.iteritems(metrics_vals)):\n",
    "            ax = fig.add_subplot(1, num_subplots, i0+1)\n",
    "            ax.set_title(k0)\n",
    "            for i1, (k1, v1) in enumerate(six.iteritems(v0)):\n",
    "                for i2, (k2, v2) in enumerate(six.iteritems(v1)):\n",
    "                      ax.plot(iterations, v2, label='%s %s' % (k1, k2),\n",
    "                      color=colors[i1], linestyle=styles[i2])\n",
    "          ax.set_xlim([1, num_iterations])\n",
    "          ax.set_yscale('log')\n",
    "          ax.legend()\n",
    "        # Plot the embeddings (first 3 dimensions).\n",
    "        ax.legend(loc='upper right')\n",
    "        ax = fig.add_subplot(1, num_subplots, num_subplots)\n",
    "        ax.scatter(\n",
    "            self._embeddings[:, 0], self._embeddings[:, 1],\n",
    "            alpha=0.5, marker='o')\n",
    "        ax.set_title('embeddings')\n",
    "\n",
    "\n",
    "  @property\n",
    "  def embeddings(self):\n",
    "    return self._embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14529b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
